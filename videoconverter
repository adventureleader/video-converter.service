#!/usr/bin/env python3.12
"""
Video Converter Service - Python 3.12 Runtime Script

A production-grade video converter service for Ubuntu 24.04 with:
- Automatic GPU detection (NVIDIA, AMD, Intel) with CPU fallback
- Real-time directory monitoring using watchdog
- Structured JSON logging with rotation
- Comprehensive error handling and retry logic
- Lockfile-based process synchronization

Usage:
    videoconverter [OPTIONS]

Options:
    --config <path>      Override config file path (default: /etc/videoconverter/config.yml)
    --log-dir <path>     Override log directory (default: /var/log/videoconverter)
    --dry-run            Log conversions without executing ffmpeg
    --debug              Enable debug logging
    --version            Show version
    --help               Show this help message

Author: Video Converter Team
Version: 1.0.0
License: MIT
"""

import argparse
import json
import logging
import logging.handlers
import os
import pathlib
import signal
import subprocess
import sys
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum
import threading
import queue

try:
    import yaml
except ImportError:
    print("ERROR: PyYAML is required. Install with: pip install PyYAML", file=sys.stderr)
    sys.exit(1)

try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler, FileModifiedEvent, FileCreatedEvent
except ImportError:
    print("ERROR: watchdog is required. Install with: pip install watchdog", file=sys.stderr)
    sys.exit(1)


# ============================================================================
# CONSTANTS AND CONFIGURATION
# ============================================================================

VERSION = "1.0.0"
DEFAULT_CONFIG_PATH = "/etc/videoconverter/config.yml"
DEFAULT_LOG_DIR = "/var/log/videoconverter"
DEFAULT_LOCKFILE = "/var/run/videoconverter.lock"
STALE_LOCK_TIMEOUT = 3600  # 1 hour in seconds
FILE_STABILITY_CHECK_INTERVAL = 2  # seconds
FILE_STABILITY_CHECK_DURATION = 5  # seconds
CONVERSION_QUEUE_TIMEOUT = 300  # 5 minutes
LOG_ROTATION_SIZE = 10 * 1024 * 1024  # 10MB
LOG_RETENTION_DAYS = 14


class GPUType(Enum):
    """Enumeration of supported GPU types."""
    NVIDIA = "nvidia"
    AMD = "amd"
    INTEL = "intel"
    CPU = "cpu"


class ConversionStatus(Enum):
    """Enumeration of conversion statuses."""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"


# ============================================================================
# CUSTOM EXCEPTIONS
# ============================================================================

class VideoConverterException(Exception):
    """Base exception for all converter errors."""
    pass


class ConfigurationError(VideoConverterException):
    """Configuration file parsing or validation error."""
    pass


class GPUDetectionError(VideoConverterException):
    """GPU detection or initialization error."""
    pass


class ConversionError(VideoConverterException):
    """FFmpeg conversion error."""
    pass


class FileHandlingError(VideoConverterException):
    """File read/write/delete error."""
    pass


class LockfileError(VideoConverterException):
    """Lockfile acquisition or management error."""
    pass


class WatchdogError(VideoConverterException):
    """Directory monitoring error."""
    pass


# ============================================================================
# CONFIGURATION CLASS
# ============================================================================

@dataclass
class Config:
    """
    Configuration manager for video converter service.
    
    Reads YAML configuration from file and supports environment variable overrides.
    Validates all required fields on initialization.
    """
    
    config_path: str
    config_data: Dict[str, Any]
    
    @classmethod
    def load(cls, config_path: Optional[str] = None) -> "Config":
        """
        Load configuration from YAML file.
        
        Args:
            config_path: Path to config file. Uses environment variable CONFIG_PATH
                        or DEFAULT_CONFIG_PATH if not provided.
        
        Returns:
            Config instance with loaded and validated configuration.
        
        Raises:
            ConfigurationError: If config file is missing, invalid, or validation fails.
        """
        # Determine config path with priority: argument > env var > default
        if config_path is None:
            config_path = os.environ.get("CONFIG_PATH", DEFAULT_CONFIG_PATH)
        
        config_path = pathlib.Path(config_path)
        
        # Check if config file exists
        if not config_path.exists():
            raise ConfigurationError(f"Configuration file not found: {config_path}")
        
        # Load YAML
        try:
            with open(config_path, 'r') as f:
                config_data = yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise ConfigurationError(f"Invalid YAML in config file: {e}")
        except IOError as e:
            raise ConfigurationError(f"Cannot read config file: {e}")
        
        if config_data is None:
            config_data = {}
        
        # Validate configuration
        cls._validate(config_data)
        
        return cls(config_path=str(config_path), config_data=config_data)
    
    @staticmethod
    def _validate(config: Dict[str, Any]) -> None:
        """
        Validate configuration structure and required fields.
        
        Args:
            config: Configuration dictionary to validate.
        
        Raises:
            ConfigurationError: If validation fails.
        """
        # Check required top-level sections
        required_sections = ["service", "directories", "logging"]
        for section in required_sections:
            if section not in config:
                raise ConfigurationError(f"Missing required section: {section}")
        
        # Validate directories section
        if "watch_paths" not in config["directories"]:
            raise ConfigurationError("Missing required field: directories.watch_paths")
        
        watch_paths = config["directories"]["watch_paths"]
        if not isinstance(watch_paths, list) or len(watch_paths) == 0:
            raise ConfigurationError("directories.watch_paths must be a non-empty list")
        
        # Validate logging section
        log_config = config["logging"]
        if "log_dir" not in log_config:
            raise ConfigurationError("Missing required field: logging.log_dir")
        
        # Validate service section
        service_config = config.get("service", {})
        if "log_level" not in service_config:
            service_config["log_level"] = "INFO"
    
    def get(self, key: str, default: Any = None) -> Any:
        """Get configuration value by dot-notation key."""
        keys = key.split(".")
        value = self.config_data
        for k in keys:
            if isinstance(value, dict):
                value = value.get(k)
            else:
                return default
        return value if value is not None else default
    
    def get_watch_paths(self) -> List[str]:
        """Get list of directories to monitor."""
        return self.config_data.get("directories", {}).get("watch_paths", [])
    
    def get_log_dir(self) -> str:
        """Get log directory path, with environment variable override."""
        return os.environ.get("LOG_PATH", 
                             self.config_data.get("logging", {}).get("log_dir", DEFAULT_LOG_DIR))
    
    def get_lockfile_path(self) -> str:
        """Get lockfile path."""
        return self.config_data.get("advanced", {}).get("lockfile", DEFAULT_LOCKFILE)
    
    def get_log_level(self) -> str:
        """Get configured log level."""
        return self.config_data.get("service", {}).get("log_level", "INFO")
    
    def get_conversion_timeout(self) -> int:
        """Get conversion timeout in seconds."""
        return self.config_data.get("service", {}).get("conversion_timeout", 3600)
    
    def get_max_workers(self) -> int:
        """Get maximum concurrent conversions."""
        return self.config_data.get("service", {}).get("max_workers", 2)
    
    def get_output_dir(self) -> str:
        """Get output directory for converted files."""
        return self.config_data.get("directories", {}).get("output_dir", "../converted")
    
    def get_delete_original(self) -> bool:
        """Check if original files should be deleted after conversion."""
        return self.config_data.get("file_handling", {}).get("delete_original", True)
    
    def get_max_retries(self) -> int:
        """Get maximum retry attempts for failed conversions."""
        return self.config_data.get("error_handling", {}).get("max_retries", 3)
    
    def get_retry_delay(self) -> int:
        """Get delay between retry attempts in seconds."""
        return self.config_data.get("error_handling", {}).get("retry_delay_seconds", 60)


# ============================================================================
# GPU DETECTION
# ============================================================================

class GPUDetector:
    """
    Detects available GPU hardware and selects optimal encoder.
    
    Checks for GPU support in this order:
    1. NVIDIA (hevc_nvenc)
    2. AMD VA-API (hevc_vaapi)
    3. Intel QSV (hevc_qsv)
    4. Fallback to CPU (libx265)
    """
    
    def __init__(self, logger: logging.Logger):
        """
        Initialize GPU detector.
        
        Args:
            logger: Logger instance for logging detection results.
        """
        self.logger = logger
        self.detected_gpu = GPUType.CPU
        self.encoder = "libx265"
    
    def detect(self) -> str:
        """
        Run GPU detection sequence.
        
        Returns:
            Name of selected encoder (hevc_nvenc, hevc_vaapi, hevc_qsv, or libx265).
        """
        self.logger.info("Starting GPU detection sequence")
        
        # Check NVIDIA
        if self._check_nvidia():
            self.detected_gpu = GPUType.NVIDIA
            self.encoder = "hevc_nvenc"
            self.logger.info("NVIDIA GPU detected, using hevc_nvenc encoder")
            return self.encoder
        
        # Check AMD VA-API
        if self._check_amd_vaapi():
            self.detected_gpu = GPUType.AMD
            self.encoder = "hevc_vaapi"
            self.logger.info("AMD VA-API GPU detected, using hevc_vaapi encoder")
            return self.encoder
        
        # Check Intel QSV
        if self._check_intel_qsv():
            self.detected_gpu = GPUType.INTEL
            self.encoder = "hevc_qsv"
            self.logger.info("Intel QSV GPU detected, using hevc_qsv encoder")
            return self.encoder
        
        # Fallback to CPU
        self.detected_gpu = GPUType.CPU
        self.encoder = "libx265"
        self.logger.warning("No GPU detected, falling back to CPU encoding (libx265)")
        return self.encoder
    
    def _check_nvidia(self) -> bool:
        """
        Check for NVIDIA GPU availability.
        
        Returns:
            True if NVIDIA GPU with HEVC support is available.
        """
        try:
            result = subprocess.run(
                ["nvidia-smi", "--query-gpu=name", "--format=csv"],
                capture_output=True,
                text=True,
                timeout=5
            )
            return result.returncode == 0 and result.stdout.strip()
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False
        except Exception as e:
            self.logger.debug(f"NVIDIA detection error: {e}")
            return False
    
    def _check_amd_vaapi(self) -> bool:
        """
        Check for AMD VA-API GPU support.
        
        Returns:
            True if AMD VA-API with HEVC support is available.
        """
        try:
            result = subprocess.run(
                "vainfo 2>/dev/null | grep -i hevc",
                shell=True,
                capture_output=True,
                text=True,
                timeout=5
            )
            return result.returncode == 0 and result.stdout.strip()
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False
        except Exception as e:
            self.logger.debug(f"AMD VA-API detection error: {e}")
            return False
    
    def _check_intel_qsv(self) -> bool:
        """
        Check for Intel QSV GPU support.
        
        Returns:
            True if Intel QSV with HEVC support is available.
        """
        try:
            result = subprocess.run(
                "ffmpeg -codecs 2>/dev/null | grep hevc_qsv",
                shell=True,
                capture_output=True,
                text=True,
                timeout=5
            )
            return result.returncode == 0 and result.stdout.strip()
        except (subprocess.TimeoutExpired, FileNotFoundError):
            return False
        except Exception as e:
            self.logger.debug(f"Intel QSV detection error: {e}")
            return False
    
    def get_gpu_type(self) -> GPUType:
        """Get detected GPU type."""
        return self.detected_gpu


# ============================================================================
# LOGGING SETUP
# ============================================================================

class JSONFormatter(logging.Formatter):
    """
    Custom JSON formatter for structured logging.
    
    Outputs log records as JSON with timestamp, level, message, and context.
    """
    
    def format(self, record: logging.LogRecord) -> str:
        """
        Format log record as JSON.
        
        Args:
            record: LogRecord to format.
        
        Returns:
            JSON-formatted log entry.
        """
        log_data = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "level": record.levelname,
            "component": record.name,
            "message": record.getMessage(),
        }
        
        # Add exception info if present
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)
        
        # Add extra fields if present
        if hasattr(record, "extra_data"):
            log_data.update(record.extra_data)
        
        return json.dumps(log_data)


class StructuredLogger:
    """
    Provides structured JSON logging with daily rotation.
    
    Creates one log file per day with format: videoconverter-YYYY-MM-DD.log
    Retains logs for 14 days.
    """
    
    def __init__(self, log_dir: str, log_level: str = "INFO"):
        """
        Initialize structured logger.
        
        Args:
            log_dir: Directory for log files.
            log_level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL).
        
        Raises:
            FileHandlingError: If log directory cannot be created.
        """
        self.log_dir = pathlib.Path(log_dir)
        self.log_level = getattr(logging, log_level.upper(), logging.INFO)
        
        # Create log directory if it doesn't exist
        try:
            self.log_dir.mkdir(parents=True, exist_ok=True)
        except OSError as e:
            raise FileHandlingError(f"Cannot create log directory {log_dir}: {e}")
        
        # Create logger
        self.logger = logging.getLogger("videoconverter")
        self.logger.setLevel(self.log_level)
        
        # Remove existing handlers
        self.logger.handlers.clear()
        
        # Create rotating file handler with daily rotation
        log_file = self.log_dir / f"videoconverter-{datetime.now().strftime('%Y-%m-%d')}.log"
        handler = logging.FileHandler(log_file)
        handler.setLevel(self.log_level)
        
        # Set JSON formatter
        formatter = JSONFormatter()
        handler.setFormatter(formatter)
        
        # Add handler to logger
        self.logger.addHandler(handler)
        
        # Also add console handler for debugging
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(self.log_level)
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)
    
    def get_logger(self) -> logging.Logger:
        """Get configured logger instance."""
        return self.logger
    
    def cleanup_old_logs(self) -> None:
        """
        Remove log files older than retention period.
        
        Retention period is defined by LOG_RETENTION_DAYS constant.
        """
        cutoff_date = datetime.now() - timedelta(days=LOG_RETENTION_DAYS)
        
        try:
            for log_file in self.log_dir.glob("videoconverter-*.log"):
                # Extract date from filename
                try:
                    file_date_str = log_file.stem.replace("videoconverter-", "")
                    file_date = datetime.strptime(file_date_str, "%Y-%m-%d")
                    
                    if file_date < cutoff_date:
                        log_file.unlink()
                        self.logger.info(f"Deleted old log file: {log_file}")
                except (ValueError, OSError):
                    pass
        except Exception as e:
            self.logger.warning(f"Error cleaning up old logs: {e}")


# ============================================================================
# LOCKFILE MANAGEMENT
# ============================================================================

class StateManager:
    """
    Manages process state and lockfile.
    
    Ensures only one instance of the service runs at a time.
    Detects and cleans up stale locks.
    """
    
    def __init__(self, lockfile_path: str, logger: logging.Logger):
        """
        Initialize state manager.
        
        Args:
            lockfile_path: Path to lockfile.
            logger: Logger instance.
        """
        self.lockfile_path = pathlib.Path(lockfile_path)
        self.logger = logger
        self.pid = os.getpid()
    
    def acquire_lock(self) -> bool:
        """
        Acquire exclusive lock.
        
        Checks for stale locks and removes them if found.
        
        Returns:
            True if lock acquired successfully.
        
        Raises:
            LockfileError: If another instance is running or lock cannot be created.
        """
        # Check for existing lock
        if self.lockfile_path.exists():
            try:
                with open(self.lockfile_path, 'r') as f:
                    lock_data = json.load(f)
                
                lock_pid = lock_data.get("pid")
                lock_time = lock_data.get("timestamp")
                
                # Check if lock is stale
                if lock_time:
                    lock_datetime = datetime.fromisoformat(lock_time.replace("Z", "+00:00"))
                    age = (datetime.now(lock_datetime.tzinfo) - lock_datetime).total_seconds()
                    
                    if age > STALE_LOCK_TIMEOUT:
                        self.logger.warning(f"Removing stale lockfile (age: {age}s)")
                        self.lockfile_path.unlink()
                    else:
                        # Check if process is still running
                        if self._is_process_running(lock_pid):
                            raise LockfileError(
                                f"Another instance is running (PID: {lock_pid})"
                            )
                        else:
                            self.logger.info("Removing lockfile for dead process")
                            self.lockfile_path.unlink()
            except json.JSONDecodeError:
                self.logger.warning("Invalid lockfile format, removing")
                self.lockfile_path.unlink()
            except Exception as e:
                self.logger.error(f"Error checking lockfile: {e}")
                raise LockfileError(f"Cannot check lockfile: {e}")
        
        # Create new lockfile
        try:
            self.lockfile_path.parent.mkdir(parents=True, exist_ok=True)
            
            lock_data = {
                "pid": self.pid,
                "timestamp": datetime.utcnow().isoformat() + "Z",
                "hostname": os.uname().nodename,
                "version": VERSION
            }
            
            with open(self.lockfile_path, 'w') as f:
                json.dump(lock_data, f, indent=2)
            
            self.logger.info(f"Lockfile acquired: {self.lockfile_path}")
            return True
        except OSError as e:
            raise LockfileError(f"Cannot create lockfile: {e}")
    
    def release_lock(self) -> bool:
        """
        Release lock and remove lockfile.
        
        Returns:
            True if lock released successfully.
        """
        try:
            if self.lockfile_path.exists():
                self.lockfile_path.unlink()
                self.logger.info("Lockfile released")
            return True
        except OSError as e:
            self.logger.error(f"Error releasing lockfile: {e}")
            return False
    
    @staticmethod
    def _is_process_running(pid: int) -> bool:
        """
        Check if process with given PID is still running.
        
        Args:
            pid: Process ID to check.
        
        Returns:
            True if process is running.
        """
        try:
            os.kill(pid, 0)  # Signal 0 doesn't kill, just checks if process exists
            return True
        except (OSError, ProcessLookupError):
            return False


# ============================================================================
# FFMPEG CONVERSION
# ============================================================================

@dataclass
class ConversionTask:
    """Represents a file conversion task."""
    input_path: str
    output_path: str
    encoder: str
    retry_count: int = 0
    status: ConversionStatus = ConversionStatus.PENDING


class FFmpegHandler:
    """
    Manages FFmpeg subprocess execution and monitoring.
    
    Builds FFmpeg commands based on encoder type and executes conversions.
    Captures output and handles errors.
    """
    
    def __init__(self, logger: logging.Logger, config: Config, dry_run: bool = False):
        """
        Initialize FFmpeg handler.
        
        Args:
            logger: Logger instance.
            config: Configuration instance.
            dry_run: If True, log conversions without executing ffmpeg.
        """
        self.logger = logger
        self.config = config
        self.dry_run = dry_run
    
    def convert(self, task: ConversionTask) -> Tuple[bool, Optional[str]]:
        """
        Execute FFmpeg conversion.
        
        Args:
            task: ConversionTask with input/output paths and encoder.
        
        Returns:
            Tuple of (success: bool, error_message: Optional[str])
        """
        input_path = pathlib.Path(task.input_path)
        output_path = pathlib.Path(task.output_path)
        
        # Validate input file
        if not input_path.exists():
            error_msg = f"Input file not found: {input_path}"
            self.logger.error(error_msg)
            return False, error_msg
        
        # Create output directory
        try:
            output_path.parent.mkdir(parents=True, exist_ok=True)
        except OSError as e:
            error_msg = f"Cannot create output directory: {e}"
            self.logger.error(error_msg)
            return False, error_msg
        
        # Build FFmpeg command
        cmd = self._build_command(str(input_path), str(output_path), task.encoder)
        
        self.logger.info(
            f"Starting conversion",
            extra={"extra_data": {
                "input_file": str(input_path),
                "output_file": str(output_path),
                "encoder": task.encoder,
                "dry_run": self.dry_run
            }}
        )
        
        # Execute conversion
        if self.dry_run:
            self.logger.info(f"DRY RUN: Would execute: {' '.join(cmd)}")
            return True, None
        
        success, error_msg = self._execute_subprocess(cmd)
        
        if success:
            self.logger.info(
                f"Conversion completed successfully",
                extra={"extra_data": {
                    "input_file": str(input_path),
                    "output_file": str(output_path)
                }}
            )
            
            # Delete original if configured
            if self.config.get_delete_original():
                try:
                    input_path.unlink()
                    self.logger.info(f"Deleted original file: {input_path}")
                except OSError as e:
                    self.logger.warning(f"Failed to delete original file: {e}")
        else:
            self.logger.error(
                f"Conversion failed",
                extra={"extra_data": {
                    "input_file": str(input_path),
                    "error": error_msg
                }}
            )
        
        return success, error_msg
    
    def _build_command(self, input_path: str, output_path: str, encoder: str) -> List[str]:
        """
        Build FFmpeg command with appropriate encoder flags.
        
        Args:
            input_path: Path to input file.
            output_path: Path to output file.
            encoder: Encoder name (hevc_nvenc, hevc_vaapi, hevc_qsv, libx265).
        
        Returns:
            List of command arguments for subprocess.
        """
        cmd = ["ffmpeg", "-i", input_path]
        
        # Add encoder-specific parameters
        if encoder == "hevc_nvenc":
            cmd.extend([
                "-c:v", "hevc_nvenc",
                "-preset", "slow",  # fast, medium, slow
                "-rc", "vbr",  # Variable bitrate
                "-cq", "28"  # Quality (0-51, lower is better)
            ])
        elif encoder == "hevc_vaapi":
            cmd.extend([
                "-c:v", "hevc_vaapi",
                "-vaapi_device", "/dev/dri/renderD128",
                "-rc_mode", "CQP",
                "-global_quality", "28"
            ])
        elif encoder == "hevc_qsv":
            cmd.extend([
                "-c:v", "hevc_qsv",
                "-preset", "slow",  # veryfast, fast, medium, slow
                "-global_quality", "28"
            ])
        else:  # libx265 (CPU)
            cmd.extend([
                "-c:v", "libx265",
                "-preset", "medium",  # ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow
                "-crf", "28"  # Quality (0-51, lower is better)
            ])
        
        # Audio codec
        cmd.extend([
            "-c:a", "aac",
            "-b:a", "128k"
        ])
        
        # Output format and options
        cmd.extend([
            "-y",  # Overwrite output file
            output_path
        ])
        
        return cmd
    
    def _execute_subprocess(self, cmd: List[str]) -> Tuple[bool, Optional[str]]:
        """
        Execute FFmpeg subprocess and capture output.
        
        Args:
            cmd: Command arguments list.
        
        Returns:
            Tuple of (success: bool, error_message: Optional[str])
        """
        try:
            timeout = self.config.get_conversion_timeout()
            
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            try:
                stdout, stderr = process.communicate(timeout=timeout)
            except subprocess.TimeoutExpired:
                process.kill()
                stdout, stderr = process.communicate()
                error_msg = f"Conversion timeout after {timeout} seconds"
                self.logger.error(error_msg)
                return False, error_msg
            
            if process.returncode == 0:
                return True, None
            else:
                error_msg = f"FFmpeg error (exit code {process.returncode}): {stderr}"
                return False, error_msg
        
        except Exception as e:
            error_msg = f"Subprocess execution error: {e}"
            self.logger.error(error_msg)
            return False, error_msg


# ============================================================================
# WATCHDOG EVENT HANDLER
# ============================================================================

class VideoFileEventHandler(FileSystemEventHandler):
    """
    Handles file system events from watchdog.
    
    Detects new and modified .mkv files, checks for stability,
    and queues them for conversion.
    """
    
    def __init__(self, conversion_queue: queue.Queue, logger: logging.Logger, config: Config):
        """
        Initialize event handler.
        
        Args:
            conversion_queue: Queue for conversion tasks.
            logger: Logger instance.
            config: Configuration instance.
        """
        super().__init__()
        self.conversion_queue = conversion_queue
        self.logger = logger
        self.config = config
        self.processing_files = set()
    
    def on_created(self, event: FileCreatedEvent) -> None:
        """
        Handle file creation event.
        
        Args:
            event: FileCreatedEvent from watchdog.
        """
        if event.is_directory:
            return
        
        file_path = pathlib.Path(event.src_path)
        
        # Check if file matches pattern
        if not self._matches_pattern(file_path):
            return
        
        self.logger.debug(f"File created: {file_path}")
        
        # Check file stability
        if self._is_stable(file_path):
            self._queue_conversion(file_path)
    
    def on_modified(self, event: FileModifiedEvent) -> None:
        """
        Handle file modification event.
        
        Args:
            event: FileModifiedEvent from watchdog.
        """
        if event.is_directory:
            return
        
        file_path = pathlib.Path(event.src_path)
        
        # Check if file matches pattern
        if not self._matches_pattern(file_path):
            return
        
        # Skip if already processing
        if str(file_path) in self.processing_files:
            return
        
        self.logger.debug(f"File modified: {file_path}")
        
        # Check file stability
        if self._is_stable(file_path):
            self._queue_conversion(file_path)
    
    def _matches_pattern(self, file_path: pathlib.Path) -> bool:
        """
        Check if file matches configured patterns.
        
        Args:
            file_path: Path to file.
        
        Returns:
            True if file matches pattern.
        """
        patterns = self.config.config_data.get("directories", {}).get("file_patterns", ["*.mkv"])
        
        for pattern in patterns:
            if file_path.match(pattern):
                return True
        
        return False
    
    def _is_stable(self, file_path: pathlib.Path) -> bool:
        """
        Check if file size is stable (not being written).
        
        Monitors file size over FILE_STABILITY_CHECK_DURATION seconds.
        File is considered stable if size doesn't change.
        
        Args:
            file_path: Path to file.
        
        Returns:
            True if file is stable.
        """
        try:
            initial_size = file_path.stat().st_size
            
            # Check file size stability
            for _ in range(FILE_STABILITY_CHECK_DURATION // FILE_STABILITY_CHECK_INTERVAL):
                time.sleep(FILE_STABILITY_CHECK_INTERVAL)
                
                if not file_path.exists():
                    return False
                
                current_size = file_path.stat().st_size
                
                if current_size != initial_size:
                    self.logger.debug(f"File still being written: {file_path}")
                    return False
            
            return True
        
        except OSError as e:
            self.logger.warning(f"Error checking file stability: {e}")
            return False
    
    def _queue_conversion(self, file_path: pathlib.Path) -> None:
        """
        Queue file for conversion.
        
        Args:
            file_path: Path to file to convert.
        """
        if str(file_path) in self.processing_files:
            return
        
        # Determine output path
        output_dir = self.config.get_output_dir()
        
        if pathlib.Path(output_dir).is_absolute():
            output_path = pathlib.Path(output_dir) / file_path.name
        else:
            output_path = file_path.parent / output_dir / file_path.name
        
        # Create task
        task = ConversionTask(
            input_path=str(file_path),
            output_path=str(output_path),
            encoder="auto"  # Will be set by main loop
        )
        
        try:
            self.conversion_queue.put(task, timeout=CONVERSION_QUEUE_TIMEOUT)
            self.processing_files.add(str(file_path))
            self.logger.info(f"Queued for conversion: {file_path}")
        except queue.Full:
            self.logger.warning(f"Conversion queue full, skipping: {file_path}")


# ============================================================================
# MAIN SERVICE LOOP
# ============================================================================

class VideoConverterService:
    """
    Main video converter service.
    
    Orchestrates GPU detection, directory monitoring, and conversion processing.
    """
    
    def __init__(self, config: Config, logger: logging.Logger, dry_run: bool = False):
        """
        Initialize service.
        
        Args:
            config: Configuration instance.
            logger: Logger instance.
            dry_run: If True, log conversions without executing ffmpeg.
        """
        self.config = config
        self.logger = logger
        self.dry_run = dry_run
        self.running = False
        self.conversion_queue = queue.Queue()
        self.observer = None
        self.state_manager = None
        self.gpu_detector = None
        self.ffmpeg_handler = None
    
    def start(self) -> None:
        """
        Start the service.
        
        Initializes all components and starts monitoring.
        
        Raises:
            VideoConverterException: If initialization fails.
        """
        self.logger.info(f"Starting Video Converter Service v{VERSION}")
        
        # Acquire lockfile
        try:
            lockfile_path = self.config.get_lockfile_path()
            self.state_manager = StateManager(lockfile_path, self.logger)
            self.state_manager.acquire_lock()
        except LockfileError as e:
            self.logger.error(f"Failed to acquire lock: {e}")
            raise
        
        # Detect GPU
        try:
            self.gpu_detector = GPUDetector(self.logger)
            encoder = self.gpu_detector.detect()
        except Exception as e:
            self.logger.error(f"GPU detection failed: {e}")
            raise
        
        # Initialize FFmpeg handler
        self.ffmpeg_handler = FFmpegHandler(self.logger, self.config, self.dry_run)
        
        # Start directory monitoring
        try:
            self._start_monitoring()
        except WatchdogError as e:
            self.logger.error(f"Failed to start monitoring: {e}")
            raise
        
        self.running = True
        self.logger.info("Service started successfully")
    
    def stop(self) -> None:
        """
        Stop the service gracefully.
        
        Stops monitoring, releases lockfile, and cleans up resources.
        """
        self.logger.info("Stopping service")
        self.running = False
        
        # Stop observer
        if self.observer:
            self.observer.stop()
            self.observer.join(timeout=5)
        
        # Release lockfile
        if self.state_manager:
            self.state_manager.release_lock()
        
        self.logger.info("Service stopped")
    
    def _start_monitoring(self) -> None:
        """
        Start watchdog directory monitoring.
        
        Raises:
            WatchdogError: If monitoring cannot be started.
        """
        try:
            watch_paths = self.config.get_watch_paths()
            
            if not watch_paths:
                raise WatchdogError("No watch paths configured")
            
            # Create event handler
            event_handler = VideoFileEventHandler(
                self.conversion_queue,
                self.logger,
                self.config
            )
            
            # Create observer
            self.observer = Observer()
            
            # Add watches for each path
            for watch_path in watch_paths:
                watch_path_obj = pathlib.Path(watch_path)
                
                if not watch_path_obj.exists():
                    self.logger.warning(f"Watch path does not exist: {watch_path}")
                    continue
                
                recursive = self.config.config_data.get("directories", {}).get("recursive", True)
                self.observer.schedule(event_handler, str(watch_path_obj), recursive=recursive)
                self.logger.info(f"Monitoring directory: {watch_path}")
            
            # Start observer
            self.observer.start()
        
        except Exception as e:
            raise WatchdogError(f"Failed to start monitoring: {e}")
    
    def run(self) -> None:
        """
        Main service loop.
        
        Processes conversion queue and handles signals.
        """
        self.start()
        
        try:
            while self.running:
                try:
                    # Get task from queue with timeout
                    task = self.conversion_queue.get(timeout=1)
                    
                    # Set encoder if not already set
                    if task.encoder == "auto":
                        task.encoder = self.gpu_detector.encoder
                    
                    # Execute conversion
                    success, error_msg = self.ffmpeg_handler.convert(task)
                    
                    if success:
                        task.status = ConversionStatus.COMPLETED
                    else:
                        task.status = ConversionStatus.FAILED
                        
                        # Retry logic
                        max_retries = self.config.get_max_retries()
                        if task.retry_count < max_retries:
                            task.retry_count += 1
                            retry_delay = self.config.get_retry_delay()
                            self.logger.info(
                                f"Retrying conversion (attempt {task.retry_count}/{max_retries}) "
                                f"after {retry_delay}s"
                            )
                            time.sleep(retry_delay)
                            self.conversion_queue.put(task)
                        else:
                            self.logger.error(
                                f"Conversion failed after {max_retries} retries: {error_msg}"
                            )
                
                except queue.Empty:
                    # No tasks in queue, continue
                    continue
                except Exception as e:
                    self.logger.error(f"Error processing conversion task: {e}")
        
        except KeyboardInterrupt:
            self.logger.info("Received interrupt signal")
        finally:
            self.stop()


# ============================================================================
# SIGNAL HANDLERS
# ============================================================================

def setup_signal_handlers(service: VideoConverterService) -> None:
    """
    Setup signal handlers for graceful shutdown.
    
    Args:
        service: VideoConverterService instance.
    """
    def signal_handler(signum: int, frame) -> None:
        """Handle signals."""
        signal_name = signal.Signals(signum).name
        service.logger.info(f"Received signal {signal_name}")
        service.stop()
    
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)


# ============================================================================
# CLI ARGUMENT PARSING
# ============================================================================

def parse_arguments() -> argparse.Namespace:
    """
    Parse command-line arguments.
    
    Returns:
        Parsed arguments namespace.
    """
    parser = argparse.ArgumentParser(
        prog="videoconverter",
        description="Video Converter Service - Automatic GPU-accelerated video conversion",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  videoconverter                          # Run with default config
  videoconverter --config /etc/custom.yml # Use custom config
  videoconverter --debug --dry-run        # Debug mode with dry run
  videoconverter --version                # Show version
        """
    )
    
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help=f"Path to config file (default: {DEFAULT_CONFIG_PATH})"
    )
    
    parser.add_argument(
        "--log-dir",
        type=str,
        default=None,
        help=f"Path to log directory (default: {DEFAULT_LOG_DIR})"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Log conversions without executing ffmpeg"
    )
    
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging"
    )
    
    parser.add_argument(
        "--version",
        action="version",
        version=f"%(prog)s {VERSION}"
    )
    
    return parser.parse_args()


# ============================================================================
# ENTRY POINT
# ============================================================================

def main() -> int:
    """
    Main entry point.
    
    Returns:
        Exit code (0 for success, non-zero for failure).
    """
    args = parse_arguments()
    
    try:
        # Load configuration
        config = Config.load(args.config)
        
        # Override log level if debug flag is set
        log_level = "DEBUG" if args.debug else config.get_log_level()
        
        # Override log directory if provided
        log_dir = args.log_dir or config.get_log_dir()
        
        # Setup logging
        logger_setup = StructuredLogger(log_dir, log_level)
        logger = logger_setup.get_logger()
        
        # Log startup information
        logger.info(
            f"Video Converter Service starting",
            extra={"extra_data": {
                "version": VERSION,
                "config_file": config.config_path,
                "log_level": log_level,
                "dry_run": args.dry_run
            }}
        )
        
        # Create and run service
        service = VideoConverterService(config, logger, dry_run=args.dry_run)
        setup_signal_handlers(service)
        service.run()
        
        return 0
    
    except ConfigurationError as e:
        print(f"Configuration Error: {e}", file=sys.stderr)
        return 1
    except LockfileError as e:
        print(f"Lockfile Error: {e}", file=sys.stderr)
        return 1
    except Exception as e:
        print(f"Fatal Error: {e}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    sys.exit(main())
